# Incident: 2025-04-03 10-33-00

## Summary

> [!NOTE]
> Write a summary of the incident in a few sentences. Include what happened, why, the severity of the incident and how long the impact lasted.

Between 10:33am and 10:44am on 04-03-2025, 20 users incounctered errors while ordering a pizza. It was caused by an increase in users at 10:31 and proceeding to open the menue and order pizzas. The increase in users all ordering pizza and opening the menue led to an increase in requests to the data base causing the data base to reject the orders.

The data base could not handle all of the requests and casued it to reject peoples pizza orders. The even was detected by Garfana by a rule that checkked for latency. The team started working on the event by checking the logs and reading the error message. This high severity incident affected 25% of users

A further impact was that we lost money and our social media page exploded with hate filled comments in relation to the incident.

## Detection

> [!NOTE]
> When did the team detect the incident? How did they know it was happening? How could we improve time-to-detection? Consider: How would we have cut that time by half?

The incident was detected when the Garfana latency alert rule was triggered and Samuel Redd was paged

Samuel did not immediatly relize what was going on and needed to read all the metrics and logs to see what was going on, delaying the response by 11 minutes.

As a company we should add more alert rules so we can know immediatly what triggered the alert and know where the problem is.

## Impact

> [!NOTE]
> Describe how the incident impacted internal and external users during the incident. Include how many support cases were raised.

For 11 minutes between 10:33 and 10:44 on 04/04/2025 our database could not accept any order requests preventing our users from ordering pizza.

This incident affected 2 customers (25% of system users), who were unable to order a pizza from our website.

## Timeline

> [!NOTE]
> Detail the incident timeline. We recommend using UTC to standardize for timezones.
> Include any notable lead-up events, any starts of activity, the first known impact, and escalations. Note any decisions or changed made, and when the incident ended, along with any post-impact events of note.

- _10:31_ - Increase in users loggin in
- _10:31_ - The users start looking at the menue and ordering pizzas
- _10:32_ - Number of pizza order faliures start increasing
- _10:33_ - Garfana Latency alert is sent
- _10:36_ - Samuel Redd opens the logs and metrics
- _10:37_ - Samuel Redd sees that Pizza Order fails is growing
- _10:38_ - Samuel Redd starts reading the logs to read the error message
- _10:42_ - Samuel Redd inds the logs that relate to 500 errors
- _10:43_ - Samuel Redd finds the url in the error message
- _10:44_ - Samuel Redd follows the url and the chaos ends
- _10:45_ - Samuel Redd watches the logs and metrics to verify that the incident has ended
- _10:45_ - Samuel Redd starts incident reports and telling customers that the incident is over

## Response

> [!NOTE]
> Who responded to the incident? When did they respond, and what did they do? Note any delays or obstacles to responding.

After reciving the page as 10:33, Samuel Redd came online at 10:36 in Garfana Dashboards in the pizza dashboard.

The engineer did not recive a specific enough alert on the incident so he had to spend time trying to figure out what was going on. He was the only on call engineer.

## Root cause

> [!NOTE]
> Note the final root cause of the incident, the thing identified that needs to change in order to prevent this class of incident from happening again.

The root cause of this incident is that the data base is not elastic and can not handle lots of connections and requests at the same time.

## Resolution

> [!NOTE]
> Describe how the service was restored and the incident was deemed over. Detail how the service was successfully restored and you knew how what steps you needed to take to recovery.
> Depending on the scenario, consider these questions: How could you improve time to mitigation? How could you have cut that time by half?

By clicking the url the chaos ended, the logs and metrics were checked to deem the incident over.

## Prevention

> [!NOTE]
> Now that you know the root cause, can you look back and see any other incidents that could have the same root cause? If yes, note what mitigation was attempted in those incidents and ask why this incident occurred again.

No other incidents had the same root cause. This was a first for us.

## Action items

> [!NOTE]
> Describe the corrective action ordered to prevent this class of incident in the future. Note who is responsible and when they have to complete the work and where that work is being tracked.

1. We should implement rate limits so one person does not hit the db repeatedly in a small amount of time
2. We need to make sure our db can handel multiple connections by having it scale up
3. Set up specific garfana alerts, especially for pizza faliuers and requests being made
